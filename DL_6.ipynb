{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRhYF8Vk90Kz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Download and extract flower photos dataset\n",
        "flowers_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = \"./flower_photos\"\n",
        "urllib.request.urlretrieve(flowers_url, \"flower_photos.tgz\")\n",
        "with tarfile.open(\"flower_photos.tgz\", \"r:gz\") as tar:\n",
        "    tar.extractall(path=data_dir)\n",
        "\n",
        "# Manually define class names based on the dataset structure\n",
        "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "print(\"Class names from the dataset:\", class_names)\n",
        "\n",
        "# Set parameters\n",
        "height, width = 128, 128  # Adjust resolution if needed\n",
        "training_batch_size = 16  # Batch size for training\n",
        "\n",
        "# Load datasets\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(height, width),\n",
        "    batch_size=training_batch_size\n",
        ")\n",
        "\n",
        "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(height, width),\n",
        "    batch_size=training_batch_size\n",
        ")\n",
        "\n",
        "# Normalize data and prefetch for faster loading\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_set = train_set.map(lambda x, y: (normalization_layer(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_set = validation_set.map(lambda x, y: (normalization_layer(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Print the number of classes\n",
        "print(\"Number of classes:\", len(class_names))\n",
        "\n",
        "# Load a pre-trained model (e.g., MobileNetV2 for efficiency) and set up custom layers\n",
        "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(height, width, 3))\n",
        "\n",
        "# Freeze layers for initial training\n",
        "for layer in base_model.layers[:120]:  # Adjust number of frozen layers as needed\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classifier layers\n",
        "x = tf.keras.layers.Flatten()(base_model.output)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "output = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)  # Use len(class_names)\n",
        "\n",
        "# Combine base model and custom layers into a new model\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile and train the classifier layers first\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_initial = model.fit(train_set, epochs=5, validation_data=validation_set)\n",
        "\n",
        "# Fine-tune with more layers unfrozen and a lower learning rate\n",
        "for layer in base_model.layers[120:]:\n",
        "    layer.trainable = True\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model.fit(train_set, epochs=2, validation_data=validation_set)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"fine_tuned_flower_model_optimized.h5\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "test_loss, test_accuracy = model.evaluate(validation_set)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Load and preprocess a sample image for prediction\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (height, width))  # Resize to match model input size\n",
        "    img = img / 255.0  # Normalize the image\n",
        "    img = img.reshape(1, height, width, 3)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "# Make sure to upload a sample image from your local device\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will prompt you to upload an image\n",
        "\n",
        "# Load the uploaded image for prediction\n",
        "sample_image_path = next(iter(uploaded))  # Get the uploaded file name\n",
        "processed_image = preprocess_image(sample_image_path)\n",
        "\n",
        "# Load the saved model for prediction\n",
        "loaded_model = tf.keras.models.load_model(\"fine_tuned_flower_model_optimized.h5\")\n",
        "\n",
        "# Make prediction\n",
        "predictions = loaded_model.predict(processed_image)\n",
        "predicted_class = class_names[tf.argmax(predictions[0]).numpy()]  # Get the name of the predicted class\n",
        "predicted_confidence = tf.reduce_max(predictions[0]).numpy()  # Get the confidence of the prediction\n",
        "\n",
        "# Display prediction results\n",
        "print(f\"Predicted Class: {predicted_class}, Confidence: {predicted_confidence:.2f}\")\n",
        "\n",
        "# Load and display the uploaded image\n",
        "sample_image = cv2.imread(sample_image_path)\n",
        "\n",
        "# Show the original image\n",
        "if sample_image is not None:\n",
        "    cv2_imshow(sample_image)  # Show the original image\n",
        "else:\n",
        "    print(\"Error: Sample image not found or could not be loaded.\")"
      ]
    }
  ]
}